# Imports:
import numpy as np
import gym

from keras.models import Sequential
from keras.backend import image_data_format
from keras.optimizers import Adam
from keras.layers import (
    Dense,
    Activation,
    Flatten,
    Convolution2D,
    Permute,
)
from rl.agents.dqn import DQNAgent
from rl.policy import (
    LinearAnnealedPolicy,
    EpsGreedyQPolicy,
)
from rl.memory import SequentialMemory

from minecraft_deep_learning.processor import MinecraftProcessor
from minecraft_deep_learning.constants import (
    ENV_NAME,
    IMAGE_SIZE,
    HIDDEN_NEURONS,
    MEMORY_SIZE,
    WINDOW_LENGTH,
    WARMUP_STEPS,
    LEARNING_RATE,
)

# Environment setup:
def build_environment():
    """Build the Minecraft gym environment."""
    env = gym.make(ENV_NAME)
    env.init(start_minecraft=True, videoResolution=IMAGE_SIZE)
    return env

# Model setup:
def build_agent(env):
    """Build an agent for the given environment."""
    num_actions = env.action_space.n

    model = Sequential([
        Permute(
            # convert (channels, width, height) input to proper ordering
            (2, 3, 1) if image_data_format() == "channels_last" else (1, 2, 3),
            input_shape=(WINDOW_LENGTH,) + IMAGE_SIZE,
        ),
        Convolution2D(16, 8, 8, subsample=(4, 4)),
        Activation("relu"),
        Convolution2D(32, 4, 4, subsample=(2, 2)),
        Activation("relu"),
        Flatten(),
        Dense(HIDDEN_NEURONS),
        Activation("relu"),
        Dense(num_actions),
    ])

    memory = SequentialMemory(
        limit=MEMORY_SIZE,
        window_length=WINDOW_LENGTH,
    )

    processor = MinecraftProcessor()

    policy = LinearAnnealedPolicy(
        EpsGreedyQPolicy(),
        attr="eps",
        value_max=1.0,
        value_min=0.1,
        value_test=0.05,  # prevents getting stuck during testing
        nb_steps=MEMORY_SIZE,
    )

    agent = DQNAgent(
        model=model,
        nb_actions=num_actions,
        policy=policy,
        memory=memory,
        processor=processor,
        nb_steps_warmup=WARMUP_STEPS,
        gamma=.99,
        target_model_update=10000,
        train_interval=4,
        delta_clip=1.0,
    )
    agent.compile(
        optimizer=Adam(lr=LEARNING_RATE),
        metrics=["mae"],
    )
    return agent
