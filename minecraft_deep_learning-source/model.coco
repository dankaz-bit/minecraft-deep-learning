# Imports:
import numpy as np

from keras.models import Sequential
from keras.backend import image_data_format
from keras.optimizers import Nadam
from keras.layers import (
    Dense,
    Activation,
    Flatten,
    Conv2D,
    Permute,
)
from rl.agents.dqn import DQNAgent
from rl.policy import (
    LinearAnnealedPolicy,
    EpsGreedyQPolicy,
)
from rl.memory import SequentialMemory

from minecraft_deep_learning.processor import MinecraftProcessor
from minecraft_deep_learning.constants import (
    INPUT_SIZE,
    HIDDEN_LAYERS,
    MEMORY_SIZE,
    WINDOW_SIZE,
    WARMUP_STEPS,
    CHANNELS,
)

# Model setup:
def build_agent(env):
    """Build an agent for the given environment."""
    num_actions = env.action_space.n

    model = Sequential([
        Permute(
            # convert (channels, width, height) input to proper ordering
            (2, 3, 1) if image_data_format() == "channels_last" else (1, 2, 3),
            input_shape=(CHANNELS * WINDOW_SIZE,) + INPUT_SIZE,
        ),
        Conv2D(16, (8, 8), strides=(4, 4), activation="relu"),
        Conv2D(32, (4, 4), strides=(2, 2), activation="relu"),
        Flatten(),
    ] + [
        Dense(neurons, activation="relu")
        for neurons in HIDDEN_LAYERS
    ] + [
        Dense(num_actions),
    ])

    memory = SequentialMemory(
        limit=MEMORY_SIZE,
        window_length=WINDOW_SIZE,
    )

    processor = MinecraftProcessor()

    policy = LinearAnnealedPolicy(
        EpsGreedyQPolicy(),
        attr="eps",
        value_max=1.0,
        value_min=0.1,
        value_test=0.05,  # prevents getting stuck during testing
        nb_steps=MEMORY_SIZE,
    )

    agent = DQNAgent(
        model=model,
        nb_actions=num_actions,
        policy=policy,
        memory=memory,
        processor=processor,
        nb_steps_warmup=WARMUP_STEPS,
        train_interval=WINDOW_SIZE,
    )
    agent.compile(
        optimizer=Nadam(),
        metrics=["mae"],
    )
    return agent
