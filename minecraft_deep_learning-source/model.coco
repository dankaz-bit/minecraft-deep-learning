# Imports:
from keras.models import Sequential
from keras.backend import image_data_format
from keras.optimizers import Nadam
from keras.layers import (
    Dense,
    Flatten,
    Conv2D,
    Permute,
)
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannGumbelQPolicy
from rl.memory import SequentialMemory

from minecraft_deep_learning.processor import MinecraftProcessor
from minecraft_deep_learning.constants import (
    INPUT_SIZE,
    DENSE_LAYERS,
    CONV_LAYERS,
    MEMORY_SIZE,
    WINDOW_SIZE,
    WARMUP_STEPS,
    CHANNELS,
    DUELING,
    DOUBLE_DQN,
    TARGET_UPDATE,
)

# Model setup:
def build_agent(env):
    """Build an agent for the given environment."""
    num_actions = env.action_space.n

    model = Sequential([
        Permute(
            # convert (channels, width, height) input to proper ordering
            (2, 3, 1) if image_data_format() == "channels_last" else (1, 2, 3),
            input_shape=(CHANNELS * WINDOW_SIZE,) + INPUT_SIZE,
        ),
    ] + [
        Conv2D(filters, size, strides=strides, activation="relu")
        for filters, size, strides in CONV_LAYERS
    ] + [
        Flatten(),
    ] + [
        Dense(neurons, activation="relu")
        for neurons in DENSE_LAYERS
    ] + [
        Dense(num_actions),
    ])

    policy = BoltzmannGumbelQPolicy()

    memory = SequentialMemory(
        limit=MEMORY_SIZE,
        window_length=WINDOW_SIZE,
    )

    processor = MinecraftProcessor()

    agent = DQNAgent(
        model=model,
        nb_actions=num_actions,
        policy=policy,
        memory=memory,
        processor=processor,
        nb_steps_warmup=WARMUP_STEPS,
        train_interval=WINDOW_SIZE,
        enable_dueling_network=DUELING,
        enable_double_dqn=DOUBLE_DQN,
        target_model_update=TARGET_UPDATE,
    )
    agent.compile(
        optimizer=Nadam(),
        metrics=["mae"],
    )
    return agent
