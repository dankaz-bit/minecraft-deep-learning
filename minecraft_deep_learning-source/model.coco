# Imports:
from pprint import pprint

import tensorflow as tf
from keras.models import Sequential
from keras.backend import image_data_format
from keras.optimizers import Adam
from keras.layers import (
    Dense,
    Flatten,
    Conv2D,
    Permute,
    Lambda,
)
from rl.agents.dqn import DQNAgent
from rl.memory import SequentialMemory
from rl.policy import (
    BoltzmannGumbelQPolicy,
    BoltzmannQPolicy,
)

from minecraft_deep_learning.processor import MinecraftProcessor
from minecraft_deep_learning.constants import (
    INPUT_SIZE,
    DENSE_LAYERS,
    CONV_LAYERS,
    MEMORY_SIZE,
    WINDOW_SIZE,
    WARMUP_STEPS,
    CHANNELS,
    DUELING,
    DOUBLE_DQN,
    TARGET_UPDATE,
    BGE_C,
    LOG_INTERVAL,
    USE_ACTIONS,
    DEBUG,
    TESTING_TAU,
)

# Model setup:
def build_agent():
    """Build an agent for the given environment."""
    num_actions = len(USE_ACTIONS)

    model = Sequential([
        Permute(
            # convert (channels, width, height) input to proper ordering
            (2, 3, 1) if image_data_format() == "channels_last" else (1, 2, 3),
            input_shape=(CHANNELS * WINDOW_SIZE,) + INPUT_SIZE,
        ),
    ] + [
        Conv2D(filters, size, strides=strides, activation="relu")
        for filters, size, strides in CONV_LAYERS
    ] + [
        Flatten(),
    ] + [
        Dense(neurons, activation="relu")
        for neurons in DENSE_LAYERS
    ] + [
        Dense(num_actions),
    ])

    policy = BoltzmannGumbelQPolicy(C=BGE_C)
    test_policy = BoltzmannQPolicy(tau=TESTING_TAU)
    print("BGE beta annealing plan: {} -> {}".format(BGE_C, TESTING_TAU))
    if DEBUG:
        policy._counter = 0
        old_select_action = policy.select_action
        def policy.select_action(q_values):
            action = old_select_action(q_values)
            if policy._counter % LOG_INTERVAL == 0:
                print()
                pprint({
                    USE_ACTIONS[action_index]: action_count
                    for action_index, action_count in enumerate(policy.action_counts)
                })
            policy._counter += 1
            return action

    memory = SequentialMemory(
        limit=MEMORY_SIZE,
        window_length=WINDOW_SIZE,
    )

    processor = MinecraftProcessor()

    agent = DQNAgent(
        model=model,
        nb_actions=num_actions,
        policy=policy,
        test_policy=test_policy,
        memory=memory,
        processor=processor,
        nb_steps_warmup=WARMUP_STEPS,
        target_model_update=TARGET_UPDATE,
        train_interval=WINDOW_SIZE,
        enable_double_dqn=DOUBLE_DQN,
        enable_dueling_network=DUELING,
    )
    agent.compile(
        optimizer=Adam(),
        metrics=["mae", "mse"],
    )
    return agent

def build_image_model(agent):
    """Construct a version of the model from agent that accepts raw images."""
    Sequential([
        Lambda(tensor ->
            tensor/255.0
            |> tensor -> [tensor]*WINDOW_SIZE
            |> tf.stack
            |> tf.transpose$(perm=[1, 0, 4, 2, 3])
            |> tf.reshape$(shape=(-1, CHANNELS * WINDOW_SIZE) + INPUT_SIZE),
            input_shape=INPUT_SIZE + (CHANNELS,)
        ),
    ] + [
        (def layer ->
            layer.inbound_nodes = [];
            layer)(layer)
        for layer in agent.model.layers
    ])
