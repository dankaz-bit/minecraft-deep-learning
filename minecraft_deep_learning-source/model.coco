# Imports:
from pprint import pprint

from keras.models import Sequential
from keras.backend import image_data_format
from keras.optimizers import Nadam
from keras.layers import (
    Dense,
    Flatten,
    Conv2D,
    Permute,
)
from rl.agents.dqn import DQNAgent
from rl.memory import SequentialMemory
from rl.policy import (
    BoltzmannGumbelQPolicy,
    BoltzmannQPolicy,
)

from minecraft_deep_learning.processor import MinecraftProcessor
from minecraft_deep_learning.constants import (
    INPUT_SIZE,
    DENSE_LAYERS,
    CONV_LAYERS,
    MEMORY_SIZE,
    WINDOW_SIZE,
    WARMUP_STEPS,
    CHANNELS,
    DUELING,
    DOUBLE_DQN,
    TARGET_UPDATE,
    BGE_C,
    LOG_INTERVAL,
    USE_ACTIONS,
    DEBUG,
    TESTING_TAU,
)

# Model setup:
def build_agent():
    """Build an agent for the given environment."""
    num_actions = len(USE_ACTIONS)

    model = Sequential([
        Permute(
            # convert (channels, width, height) input to proper ordering
            (2, 3, 1) if image_data_format() == "channels_last" else (1, 2, 3),
            input_shape=(CHANNELS * WINDOW_SIZE,) + INPUT_SIZE,
        ),
    ] + [
        Conv2D(filters, size, strides=strides, activation="relu")
        for filters, size, strides in CONV_LAYERS
    ] + [
        Flatten(),
    ] + [
        Dense(neurons, activation="relu")
        for neurons in DENSE_LAYERS
    ] + [
        Dense(num_actions),
    ])

    policy = BoltzmannGumbelQPolicy(C=BGE_C)
    test_policy = BoltzmannQPolicy(tau=TESTING_TAU)
    print("BGE beta annealing plan: {} -> {}".format(BGE_C, TESTING_TAU))
    if DEBUG:
        policy._counter = 0
        old_select_action = policy.select_action
        def policy.select_action(q_values):
            action = old_select_action(q_values)
            if policy._counter % LOG_INTERVAL == 0:
                pprint({
                    USE_ACTIONS[action_index]: action_count
                    for action_index, action_count in enumerate(policy.action_counts)
                })
            policy._counter += 1
            return action

    memory = SequentialMemory(
        limit=MEMORY_SIZE,
        window_length=WINDOW_SIZE,
    )

    processor = MinecraftProcessor()

    agent = DQNAgent(
        model=model,
        nb_actions=num_actions,
        policy=policy,
        test_policy=test_policy,
        memory=memory,
        processor=processor,
        nb_steps_warmup=WARMUP_STEPS,
        target_model_update=TARGET_UPDATE,
        train_interval=WINDOW_SIZE,
        enable_double_dqn=DOUBLE_DQN,
        enable_dueling_network=DUELING,
    )
    agent.compile(
        optimizer=Nadam(),
        metrics=["mae", "mse"],
    )
    return agent
