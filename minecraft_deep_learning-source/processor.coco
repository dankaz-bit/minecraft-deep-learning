# Imports:
import math
import os.path
from pprint import pprint

import numpy as np
from PIL import Image
from rl.core import Processor

from minecraft_deep_learning.constants import (
    IMAGE_SIZE,
    DOWNSAMPLE,
    INPUT_SIZE,
    GRAYSCALE,
    OBSERVE_DEPTH,
    CHANNELS,
    WINDOW_SIZE,
    TRIM_HEIGHT,
    DEBUG,
    LOG_INTERVAL,
    DEBUG_LOG_INTERVAL,
    USE_REWARDS,
    POSITION_POTENTIAL,
    ANGLE_POTENTIAL,
    IMAGES_DIR,
    DISCOUNT_GAMMA,
    REWARD_POTENTIAL,
)
from minecraft_deep_learning.display import (
    create_screen,
    show_array,
    pump_events,
)

# Processor:
class MinecraftProcessor(Processor):
    """Convert gym_minecraft output into a form understood by keras-rl."""
    handle_events = staticmethod(pump_events)

    def __init__(self, use_display=None, always_show_rewards=False):
        super(MinecraftProcessor, self).__init__()
        self.screen = None
        self.always_show_rewards = always_show_rewards
        self.set_prev_potential()
        if use_display ?? DEBUG:
            self.counter = -1
            self.use_display()

    def use_display(self):
        """Enable logging of observations to the screen."""
        if self.screen is None:
            self.screen_size = IMAGE_SIZE
            self.screen = create_screen(self.screen_size)

    def process_step(self, observation, reward, done, info):
        """Process a step from the environment. Called by the agent."""
        if DEBUG:
            self.counter += 1
        processed_step = (
            self.process_observation(observation),
            self.process_reward(reward, info),
            done,
            self.process_info(info),
        )
        if done:
            self.set_prev_potential()
        return processed_step

    def set_prev_potential(self, reward=None):
        """Set the previous reward for computing reward deltas."""
        self.prev_potential = reward

    def process_observation(self, observation):
        assert IMAGE_SIZE is not None and observation.shape == IMAGE_SIZE + (3 + OBSERVE_DEPTH,), observation.shape
        processed_observation = (
            observation
            |> Image.fromarray
            # PIL uses (width, height) not (height, width)
            |> (.resize(IMAGE_SIZE |> map$(-> _//DOWNSAMPLE) |> reversed |> tuple)
                if DOWNSAMPLE and DOWNSAMPLE != 1 else ->_)
            |> (.convert("L") if GRAYSCALE else ->_)
            |> np.asarray$(dtype="uint8")
        )
        if TRIM_HEIGHT:
            height = processed_observation.shape[0]
            processed_observation = processed_observation[height//2:]
        assert processed_observation.shape == INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), processed_observation.shape

        if DEBUG and self.counter % LOG_INTERVAL == 0:
            image_file = os.path.join(IMAGES_DIR, "obs_{}.png".format(self.counter))
            Image.fromarray(processed_observation).save(image_file)

        if self.screen is not None:
            display_image = (
                processed_observation
                |> Image.fromarray
                |> .convert("RGB")
                # PIL uses (width, height) not (height, width)
                |> .resize(self.screen_size |> reversed |> tuple)
                |> np.asarray$(dtype="uint8")
            )
            assert display_image.shape == self.screen_size + (3,), (display_image.shape, self.screen_size + (3,))
            show_array(self.screen, display_image)
            self.handle_events()

        return processed_observation

    def process_state_batch(self, batch):
        assert batch.shape[1:] == (WINDOW_SIZE,) + INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), batch.shape
        # We could perform this processing step in `process_observation`. In this case, however,
        # we would need to store a `float32` array instead, which is 4x more memory intensive than
        # a `uint8` array. This matters if we store 1M observations.
        processed_batch = batch.astype("float32")/255.0
        if CHANNELS > 1:
            # merge WINDOW_SIZE and CHANNELS together
            processed_batch = (
                np.moveaxis(processed_batch, -1, 2)
                |> .reshape((batch.shape[0], -1) + INPUT_SIZE)
            )
        assert processed_batch.shape == (batch.shape[0], CHANNELS * WINDOW_SIZE) + INPUT_SIZE, processed_batch.shape
        return processed_batch

    def process_info(self, info):
        # dictionary-valued infos are not supported by keras-rl
        info["observation"] = (
            list(info["observation"].items())
            if info["observation"] is not None else []
        )
        if DEBUG and self.counter % LOG_INTERVAL == 0:
            pprint(info)
        return info

    @property
    def show_rewards(self) =
        DEBUG or self.always_show_rewards

    def process_reward(self, reward, info):
        if self.show_rewards:
            reward_history = [(reward, None)]

        reward = USE_REWARDS.get(reward, 0)
        if self.show_rewards:
            reward_history.append((reward, "using"))

        if reward == 0 and (REWARD_POTENTIAL ?? False) is not False and info["observation"] is not None:
            potential = 0

            if POSITION_POTENTIAL:
                reward_coords, metric = POSITION_POTENTIAL
                x = np.asarray([
                    info["observation"][coord.upper() + "Pos"]
                    for coord in reward_coords |> sorted
                ])
                y = np.asarray([
                    pos for coord, pos in reward_coords.items() |> sorted
                ])
                potential += metric(x, y)
                if self.show_rewards:
                    reward_history.append((potential, "position {}".format({
                        coord: x[i] for i, coord in reward_coords |> sorted |> enumerate
                    })))

            if ANGLE_POTENTIAL:
                desired_x, desired_z, metric = ANGLE_POTENTIAL
                x, z, raw_yaw = ("XPos", "ZPos", "Yaw") |> map$(info["observation"][])
                # calculate yaw and desired_yaw to match up
                yaw = (raw_yaw + 90) % 360
                desired_yaw = (
                    math.atan2(z - desired_z, x - desired_x) + math.pi
                    |> math.degrees
                )
                potential += metric(yaw, desired_yaw)
                if self.show_rewards:
                    reward_history.append((potential, "got angle {}; desired angle {}".format(yaw, desired_yaw)))

            if self.prev_potential is not None:
                reward += (DISCOUNT_GAMMA + REWARD_POTENTIAL)*potential - self.prev_potential
            self.set_prev_potential(potential)
            if self.show_rewards:
                reward_history.append((reward, "reward shaping {}".format(potential)))

        if self.always_show_rewards or self.show_rewards and (
            self.counter % DEBUG_LOG_INTERVAL == 0
            or abs(reward) >= 1
        ):
            prev_reward = reward_history[0][0]
            reward_str = "reward: {}".format(prev_reward)
            for i in range(1, len(reward_history)):
                reward_i, reason = reward_history[i]
                reward_diff = reward_i - prev_reward
                if reward_diff > 0:
                    reward_str += "\t+ {}".format(reward_diff)
                elif reward_diff < 0:
                    reward_str += "\t- {}".format(-reward_diff)
                reward_str += " ({})".format(reason)
                prev_reward = reward_i
            print("\n{}\n\t= {}".format(reward_str, reward))

        return reward
