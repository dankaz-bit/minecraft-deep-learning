# Imports:
import math
import os.path
from pprint import pprint

import numpy as np
from PIL import Image
from rl.core import Processor

from minecraft_deep_learning.constants import (
    IMAGE_SIZE,
    DOWNSAMPLE,
    INPUT_SIZE,
    GRAYSCALE,
    OBSERVE_DEPTH,
    CHANNELS,
    WINDOW_SIZE,
    TRIM_HEIGHT,
    DEBUG,
    LOG_INTERVAL,
    DEBUG_LOG_INTERVAL,
    MODIFY_REWARDS,
    REWARD_POSITION,
    REWARD_ANGLE,
    REWARD_OFFSET,
    IMAGES_DIR,
)
from minecraft_deep_learning.display import (
    create_screen,
    show_array,
    pump_events,
)

# Processor:
class MinecraftProcessor(Processor):
    """Convert gym_minecraft output into a form understood by keras-rl."""
    handle_events = staticmethod(pump_events)

    def __init__(self, use_display=None, always_show_rewards=False):
        super(MinecraftProcessor, self).__init__()
        self.screen = None
        self.always_show_rewards = always_show_rewards
        if use_display ?? DEBUG:
            self.counter = -1
            self.use_display()

    def use_display(self):
        if self.screen is None:
            self.screen_size = IMAGE_SIZE
            self.screen = create_screen(self.screen_size)

    def process_step(self, observation, reward, done, info):
        if DEBUG:
            self.counter += 1
        return (
            self.process_observation(observation),
            self.process_reward(reward, info),
            done,
            self.process_info(info),
        )

    def process_observation(self, observation):
        assert IMAGE_SIZE is not None and observation.shape == IMAGE_SIZE + (3 + OBSERVE_DEPTH,), observation.shape
        processed_observation = (
            observation
            |> Image.fromarray
            # PIL uses (width, height) not (height, width)
            |> (.resize(IMAGE_SIZE |> map$(-> _//DOWNSAMPLE) |> reversed |> tuple)
                if DOWNSAMPLE and DOWNSAMPLE != 1 else ->_)
            |> (.convert("L") if GRAYSCALE else ->_)
            |> np.asarray$(dtype="uint8")
        )
        if TRIM_HEIGHT:
            height = processed_observation.shape[0]
            processed_observation = processed_observation[height//2:]
        assert processed_observation.shape == INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), processed_observation.shape

        if DEBUG and self.counter % LOG_INTERVAL == 0:
            image_file = os.path.join(IMAGES_DIR, "obs_{}.png".format(self.counter))
            Image.fromarray(processed_observation).save(image_file)

        if self.screen is not None:
            display_image = (
                processed_observation
                |> Image.fromarray
                |> .convert("RGB")
                # PIL uses (width, height) not (height, width)
                |> .resize(self.screen_size |> reversed |> tuple)
                |> np.asarray$(dtype="uint8")
            )
            assert display_image.shape == self.screen_size + (3,), (display_image.shape, self.screen_size + (3,))
            show_array(self.screen, display_image)
            self.handle_events()

        return processed_observation

    def process_state_batch(self, batch):
        assert batch.shape[1:] == (WINDOW_SIZE,) + INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), batch.shape
        # We could perform this processing step in `process_observation`. In this case, however,
        # we would need to store a `float32` array instead, which is 4x more memory intensive than
        # a `uint8` array. This matters if we store 1M observations.
        processed_batch = batch.astype("float32")/255.0
        if CHANNELS > 1:
            # merge WINDOW_SIZE and CHANNELS together
            processed_batch = (
                np.moveaxis(processed_batch, -1, 2)
                |> .reshape((batch.shape[0], -1) + INPUT_SIZE)
            )
        assert processed_batch.shape == (batch.shape[0], CHANNELS * WINDOW_SIZE) + INPUT_SIZE, processed_batch.shape
        return processed_batch

    def process_info(self, info):
        # dictionary-valued infos are not supported by keras-rl
        info["observation"] = (
            list(info["observation"].items())
            if info["observation"] is not None else []
        )
        if DEBUG and self.counter % LOG_INTERVAL == 0:
            pprint(info)
        return info

    @property
    def show_rewards(self) =
        DEBUG or self.always_show_rewards

    def process_reward(self, reward, info):
        if self.show_rewards:
            reward_history = [(reward, None)]
        if MODIFY_REWARDS and reward in MODIFY_REWARDS:
            reward = MODIFY_REWARDS[reward]
            if self.show_rewards:
                reward_history.append((reward, "modified"))
        if REWARD_OFFSET:
            reward -= REWARD_OFFSET
            if self.show_rewards:
                reward_history.append((reward, "offset"))
        if info["observation"] is not None:

            if REWARD_POSITION:
                reward_coords, metric = REWARD_POSITION
                x = np.asarray([
                    info["observation"][coord.upper() + "Pos"]
                    for coord in reward_coords |> sorted
                ])
                y = np.asarray([
                    pos for coord, pos in reward_coords.items() |> sorted
                ])
                reward += metric(x, y)
                if self.show_rewards:
                    reward_history.append((reward, "position {}".format({
                        coord: x[i] for i, coord in reward_coords |> sorted |> enumerate
                    })))

            if REWARD_ANGLE:
                desired_x, desired_z, metric = REWARD_ANGLE
                x, z, yaw = ("XPos", "ZPos", "Yaw") |> map$(info["observation"][])
                if yaw < 0:  # force positive yaw
                    yaw += 360
                desired_yaw = (
                    math.atan2(x - desired_x, z - desired_z) + math.pi  # force positive desired_yaw
                    |> math.degrees
                )
                reward += metric(yaw, desired_yaw)
                if self.show_rewards:
                    reward_history.append((reward, "got angle {}; desired angle {}".format(yaw, desired_yaw)))

        if self.always_show_rewards or self.show_rewards and (
            self.counter % DEBUG_LOG_INTERVAL == 0
            or any(abs(r) > 1 for r, why in reward_history)
        ):
            prev_reward = reward_history[0][0]
            reward_str = "\nreward: {}".format(prev_reward)
            for i in range(1, len(reward_history)):
                reward_i, reason = reward_history[i]
                reward_diff = reward_i - prev_reward
                if reward_diff >= 0:
                    reward_str += "\t+ {}".format(reward_diff)
                else:
                    reward_str += "\t- {}".format(-reward_diff)
                reward_str += " ({})".format(reason)
                prev_reward = reward_i
            print("{}\t= {}".format(reward_str, reward))
        return reward
