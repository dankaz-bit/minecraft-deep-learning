# Imports:
import math
from pprint import pprint

import numpy as np
from PIL import Image

from rl.core import Processor

from minecraft_deep_learning.constants import (
    IMAGE_SIZE,
    DOWNSAMPLE,
    INPUT_SIZE,
    GRAYSCALE,
    OBSERVE_DEPTH,
    CHANNELS,
    WINDOW_SIZE,
    TRIM_HEIGHT,
    DEBUG,
    LOG_INTERVAL,
    REWARD_POSITION,
    REWARD_POSITION_NORM,
    DISABLE_REWARDS,
)
from minecraft_deep_learning.display import (
    create_screen,
    show_array,
    pump_events,
)

# Processor:
class MinecraftProcessor(Processor):
    """Convert gym_minecraft output into a form understood by keras-rl."""
    screen = None
    handle_events = staticmethod(pump_events)

    def __init__(self, *args, **kwargs):
        if DEBUG:
            self.counter = -1
            self.use_display()
        super(MinecraftProcessor, self).__init__(*args, **kwargs)

    def use_display(self):
        if self.screen is None:
            self.screen_size = IMAGE_SIZE
            self.screen = create_screen(self.screen_size)

    def process_step(self, observation, reward, done, info):
        if DEBUG:
            self.counter += 1
        return (
            self.process_observation(observation),
            self.process_reward(reward, info),
            done,
            self.process_info(info),
        )

    def process_observation(self, observation):
        assert IMAGE_SIZE is not None and observation.shape == IMAGE_SIZE + (3 + OBSERVE_DEPTH,), observation.shape
        processed_observation = (
            observation
            |> Image.fromarray
            # PIL uses (width, height) not (height, width)
            |> (.resize(IMAGE_SIZE |> map$(-> _//DOWNSAMPLE) |> reversed |> tuple)
                if DOWNSAMPLE and DOWNSAMPLE != 1 else ->_)
            |> (.convert("L") if GRAYSCALE else ->_)
            |> np.asarray$(dtype="uint8")
        )
        if TRIM_HEIGHT:
            height = processed_observation.shape[0]
            processed_observation = processed_observation[height//2:]
        assert processed_observation.shape == INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), processed_observation.shape
        if self.screen is not None:
            display_image = (
                processed_observation
                |> Image.fromarray
                |> .convert("RGB")
                # PIL uses (width, height) not (height, width)
                |> .resize(self.screen_size |> reversed |> tuple)
                |> np.asarray$(dtype="uint8")
            )
            assert display_image.shape == self.screen_size + (3,), (display_image.shape, self.screen_size + (3,))
            show_array(self.screen, display_image)
            self.handle_events()
        return processed_observation

    def process_state_batch(self, batch):
        assert batch.shape[1:] == (WINDOW_SIZE,) + INPUT_SIZE + ((CHANNELS,) if CHANNELS > 1 else ()), batch.shape
        # We could perform this processing step in `process_observation`. In this case, however,
        # we would need to store a `float32` array instead, which is 4x more memory intensive than
        # a `uint8` array. This matters if we store 1M observations.
        processed_batch = batch.astype("float32")/255.0
        if CHANNELS > 1:
            # merge WINDOW_SIZE and CHANNELS together
            processed_batch = (
                np.moveaxis(processed_batch, -1, 2)
                |> .reshape((batch.shape[0], -1) + INPUT_SIZE)
            )
        assert processed_batch.shape == (batch.shape[0], CHANNELS * WINDOW_SIZE) + INPUT_SIZE, processed_batch.shape
        return processed_batch

    def process_info(self, info):
        # dictionary-valued infos are not supported by keras-rl
        info["observation"] = (
            list(info["observation"].items())
            if info["observation"] is not None else []
        )
        if DEBUG:
            if self.counter % LOG_INTERVAL == 0:
                pprint(info)
        return info

    def process_reward(self, reward, info):
        if DEBUG:
            reward_history = [(reward, "original")]
        if DISABLE_REWARDS and reward in DISABLE_REWARDS:
            reward = 0
            if DEBUG:
                reward_history.append.((reward, "disabled"))
        if info["observation"] is not None:
            if REWARD_POSITION:
                position_reward = 0
                for coord, (offset, factor) in REWARD_POSITION.items():
                    pos = info["observation"][coord.upper() + "Pos"]
                    position_reward += (
                        ((pos - offset)*factor)**REWARD_POSITION_NORM
                        |> math.copysign$(?, factor)
                    )
                reward += (
                    abs(position_reward)**(1/REWARD_POSITION_NORM)
                    |> math.copysign$(?, position_reward)
                )
                if DEBUG:
                    reward_history.append((reward, "position"))
        if DEBUG and (
            self.counter % (LOG_INTERVAL//10) == 0
            or any(r >= 100 for r, why in reward_history)
        ):
            print("\nreward: " + " -> ".join("{} ({})".format(r, why) for r, why in reward_history))
        return reward
